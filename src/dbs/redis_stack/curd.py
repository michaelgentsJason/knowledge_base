"""
redis stackçš„å¢åˆ æ”¹æŸ¥é€»è¾‘ - è¿›ä¸€æ­¥ä¼˜åŒ–ç‰ˆ
æ·»åŠ äº†æ‰¹é‡æ“ä½œã€ç¼“å­˜ã€è¿æ¥æ± ç­‰ä¼˜åŒ–
"""
import redis.asyncio as redis
import asyncio
from typing import List, Dict, Any, Optional
import json
from datetime import datetime, timedelta

from src.utils.logger import logger
from src.dbs.redis_stack.init import redis_client
from src.service.config import my_config

cfg = my_config.get_redis_config()
client = redis_client.get_client()
assert isinstance(client, redis.Redis)

# ç¼“å­˜é…ç½®
CACHE_TTL = 300  # ç¼“å­˜5åˆ†é’Ÿ
STATS_CACHE_KEY = "hotspot:stats:{group_id}"
INDEX_STATUS_CACHE = "hotspot:index_status"

# è¾…åŠ©å‡½æ•°

def parse_search_result(result):
    """è¾…åŠ©å‡½æ•° è§£æredisçš„æœç´¢ç»“æœ"""
    if not result or len(result) < 2:
        return []

    parsed_results = []
    for i in range(1, len(result), 2):
        if i + 1 < len(result):
            key = result[i]
            fields = result[i + 1]

            result_dict = {'key': key}
            for j in range(0, len(fields), 2):
                if j + 1 < len(fields):
                    field_name = fields[j]
                    field_value = fields[j + 1]
                    result_dict[field_name] = field_value

            parsed_results.append(result_dict)

    return parsed_results


def parse_vector_search_result(result):
    """è§£æå‘é‡æœç´¢ç»“æœï¼ŒåŒ…å«ç›¸ä¼¼åº¦è¯„åˆ†"""
    if not result or len(result) < 2:
        return []

    parsed_results = []

    for i in range(1, len(result), 2):
        if i + 1 < len(result):
            key = result[i]
            fields = result[i + 1]

            result_dict = {'key': key}
            similarity_score = None

            for j in range(0, len(fields), 2):
                if j + 1 < len(fields):
                    field_name = fields[j]
                    field_value = fields[j + 1]

                    if field_name == '__vector_score':
                        # è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼ˆ1 - è·ç¦»ï¼‰
                        similarity_score = 1.0 - float(field_value)
                    else:
                        result_dict[field_name] = field_value

            if similarity_score is not None:
                result_dict['similarity_score'] = round(similarity_score, 4)

            parsed_results.append(result_dict)

    return parsed_results


# ç´¢å¼•ç®¡ç†ä¼˜åŒ–

async def create_hotspot_index(group_id: str, force_recreate: bool = False):
    """
    åˆ›å»ºçƒ­ç‚¹é—®é¢˜å‘é‡ç´¢å¼•
    :param group_id: åˆ†ç»„ID
    :param force_recreate: æ˜¯å¦å¼ºåˆ¶é‡å»ºç´¢å¼•
    """
    try:
        # æ£€æŸ¥ç´¢å¼•çŠ¶æ€ç¼“å­˜
        if not force_recreate:
            cached_status = await client.get(f"{INDEX_STATUS_CACHE}:{group_id}")
            if cached_status == "active":
                logger.info(f"ç´¢å¼• {group_id} å·²å­˜åœ¨ä¸”æ­£å¸¸ï¼Œè·³è¿‡åˆ›å»º")
                return True

        # å°è¯•åˆ é™¤æ—§ç´¢å¼•
        try:
            await client.execute_command('FT.DROPINDEX', group_id)
            logger.info(f"ğŸ—‘ï¸ åˆ é™¤æ—§ç´¢å¼•: {group_id}")
        except Exception:
            logger.info(f"ç´¢å¼• {group_id} ä¸å­˜åœ¨ï¼Œå‡†å¤‡åˆ›å»ºæ–°ç´¢å¼•")

        # åˆ›å»ºæ–°ç´¢å¼•
        await client.execute_command(
            'FT.CREATE', group_id,
            'ON', 'JSON',
            'PREFIX', '1', group_id,
            'SCHEMA',
            '$.query_vector', 'AS', 'vector', 'VECTOR', 'FLAT', '6',
            'TYPE', 'FLOAT32', 'DIM', str(cfg.get('n_dim', 1024)),
            'DISTANCE_METRIC', 'COSINE',
            '$.category', 'AS', 'category', 'TAG',
            '$.question', 'AS', 'question', 'TEXT',
            '$.question_id', 'AS', 'question_id', 'TEXT',
            '$.created_at', 'AS', 'created_at', 'TEXT',
            '$.updated_at', 'AS', 'updated_at', 'TEXT'
        )

        # ç¼“å­˜ç´¢å¼•çŠ¶æ€
        await client.setex(f"{INDEX_STATUS_CACHE}:{group_id}", CACHE_TTL, "active")

        logger.info(f"âœ… ç´¢å¼• {group_id} åˆ›å»ºæˆåŠŸ (ç»´åº¦: {cfg.get('n_dim', 1024)})")
        return True

    except Exception as e:
        logger.error(f"âŒ ç´¢å¼• {group_id} åˆ›å»ºå¤±è´¥: {e}")
        return False


async def check_index_exists(group_id: str) -> bool:
    """æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨"""
    try:
        await client.execute_command('FT.INFO', group_id)
        return True
    except Exception:
        return False


# æ‰¹é‡æ“ä½œä¼˜åŒ–

async def store_hotspot_questions_batch(questions_data: List[Dict], group_id: str):
    """
    æ‰¹é‡å­˜å‚¨çƒ­ç‚¹é—®é¢˜ - ä½¿ç”¨pipelineä¼˜åŒ–
    :param questions_data: é—®é¢˜æ•°æ®åˆ—è¡¨
    :param group_id: åˆ†ç»„ID
    :return: (æˆåŠŸæ•°é‡, å¤±è´¥æ•°é‡, å¤±è´¥è¯¦æƒ…)
    """
    if not questions_data:
        return 0, 0, []

    success_count = 0
    failed_items = []

    try:
        # ä½¿ç”¨pipelineæ‰¹é‡æ“ä½œ
        pipe = client.pipeline()

        # éªŒè¯æ•°æ®æ ¼å¼
        required_keys = {"query_vector", "category", "question", "question_id"}
        valid_items = []

        for i, data in enumerate(questions_data):
            if not isinstance(data, dict) or not required_keys.issubset(data.keys()):
                failed_items.append({
                    "index": i,
                    "question_id": data.get("question_id", f"unknown_{i}"),
                    "reason": "æ•°æ®æ ¼å¼ä¸ç¬¦åˆè¦æ±‚"
                })
                continue
            valid_items.append((i, data))

        # æ‰¹é‡è®¾ç½®JSONæ•°æ®
        for original_index, data in valid_items:
            key = f"{group_id}{data['question_id']}"
            pipe.json().set(key, '$', data)

        # æ‰§è¡Œæ‰¹é‡æ“ä½œ
        if valid_items:
            results = await pipe.execute()

            # å¤„ç†ç»“æœ
            for (original_index, data), result in zip(valid_items, results):
                if result:  # Redis JSON.SET æˆåŠŸè¿”å› 'OK'
                    success_count += 1
                else:
                    failed_items.append({
                        "index": original_index,
                        "question_id": data["question_id"],
                        "reason": "Rediså­˜å‚¨å¤±è´¥"
                    })

        logger.info(f"æ‰¹é‡å­˜å‚¨å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {len(failed_items)}")
        return success_count, len(failed_items), failed_items

    except Exception as e:
        logger.error(f"æ‰¹é‡å­˜å‚¨å¼‚å¸¸: {e}")
        # å¦‚æœæ‰¹é‡æ“ä½œå¤±è´¥ï¼Œå°è¯•é€ä¸ªå­˜å‚¨
        return await _fallback_store_individual(questions_data, group_id)


async def _fallback_store_individual(questions_data: List[Dict], group_id: str):
    """æ‰¹é‡æ“ä½œå¤±è´¥æ—¶çš„å›é€€æ–¹æ¡ˆï¼šé€ä¸ªå­˜å‚¨"""
    logger.warning("æ‰¹é‡æ“ä½œå¤±è´¥ï¼Œå›é€€åˆ°é€ä¸ªå­˜å‚¨æ¨¡å¼")

    success_count = 0
    failed_items = []

    for i, data in enumerate(questions_data):
        try:
            success = await store_hotspot_question(
                question_id=data["question_id"],
                data=data,
                group_id=group_id
            )
            if success:
                success_count += 1
            else:
                failed_items.append({
                    "index": i,
                    "question_id": data["question_id"],
                    "reason": "å•ä¸ªå­˜å‚¨å¤±è´¥"
                })
        except Exception as e:
            failed_items.append({
                "index": i,
                "question_id": data.get("question_id", f"unknown_{i}"),
                "reason": str(e)
            })

    return success_count, len(failed_items), failed_items


async def store_hotspot_question(question_id: str, data: dict, group_id: str):
    """å­˜å‚¨å•ä¸ªçƒ­ç‚¹é—®é¢˜"""
    key = f"{group_id}{question_id}"

    try:
        required_keys = {"query_vector", "category", "question", "question_id"}
        if not isinstance(data, dict) or not required_keys.issubset(data.keys()):
            logger.warning(f"æ•°æ®æ ¼å¼ä¸ç¬¦åˆè¦æ±‚: {question_id}")
            return False

        await client.json().set(key, '$', data)
        logger.debug(f"âœ… å­˜å‚¨é—®é¢˜: {question_id}")
        return True

    except Exception as e:
        logger.error(f"âŒ å­˜å‚¨å¤±è´¥ {question_id}: {e}")
        return False


# æœç´¢åŠŸèƒ½ä¼˜åŒ–

async def vector_search_questions(
    group_id: str,
    query_vector: List[float],
    limit: int = 3,
    category: str = None,
    min_similarity: float = 0.0
):
    """
    ä¼˜åŒ–çš„å‘é‡æœç´¢
    :param group_id: åˆ†ç»„ID
    :param query_vector: æŸ¥è¯¢å‘é‡
    :param limit: è¿”å›æ•°é‡é™åˆ¶
    :param category: å¯é€‰çš„åˆ†ç±»è¿‡æ»¤
    :param min_similarity: æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
    """
    try:
        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        if not await check_index_exists(group_id):
            logger.warning(f"ç´¢å¼• {group_id} ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»º")
            await create_hotspot_index(group_id)

        # æ„å»ºå‘é‡å­—ç¬¦ä¸²
        vector_blob = ','.join(map(str, query_vector))

        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        if category:
            query = f"@category:{{{category}}}=>[KNN {limit} @vector $query_vector AS __vector_score]"
        else:
            query = f"*=>[KNN {limit} @vector $query_vector AS __vector_score]"

        logger.debug(f"æ‰§è¡Œå‘é‡æœç´¢: group_id={group_id}, limit={limit}, category={category}")

        result = await client.execute_command(
            'FT.SEARCH', group_id, query,
            'PARAMS', '2', 'query_vector', vector_blob,
            'SORTBY', '__vector_score',
            'RETURN', '8', 'question_id', 'question', 'standard_reply', 'category',
            'related_links', 'created_at', 'updated_at', '__vector_score',
            'DIALECT', '2'
        )

        parsed_results = parse_vector_search_result(result)

        # åº”ç”¨ç›¸ä¼¼åº¦è¿‡æ»¤
        if min_similarity > 0:
            filtered_results = [
                r for r in parsed_results
                if r.get('similarity_score', 0) >= min_similarity
            ]
            logger.debug(f"ç›¸ä¼¼åº¦è¿‡æ»¤: {len(parsed_results)} -> {len(filtered_results)}")
            parsed_results = filtered_results

        logger.info(f"å‘é‡æœç´¢å®Œæˆ: è¿”å› {len(parsed_results)} ä¸ªç»“æœ")
        return parsed_results

    except Exception as e:
        logger.error(f"âŒ å‘é‡æœç´¢å¤±è´¥: {e}")
        return []


# ç¼“å­˜ä¼˜åŒ–çš„ç»Ÿè®¡åŠŸèƒ½

async def get_stats(group_id: str, use_cache: bool = True):
    """
    è·å–ç»Ÿè®¡ä¿¡æ¯ - æ”¯æŒç¼“å­˜
    :param group_id: åˆ†ç»„ID
    :param use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
    """
    cache_key = STATS_CACHE_KEY.format(group_id=group_id)

    try:
        # å°è¯•ä»ç¼“å­˜è·å–
        if use_cache:
            cached_stats = await client.get(cache_key)
            if cached_stats:
                try:
                    stats = json.loads(cached_stats)
                    logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„ç»Ÿè®¡ä¿¡æ¯: {group_id}")
                    return stats
                except json.JSONDecodeError:
                    logger.warning(f"ç¼“å­˜æ•°æ®è§£æå¤±è´¥: {group_id}")

        # é‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = await _calculate_stats(group_id)

        # ç¼“å­˜ç»“æœ
        if use_cache and stats:
            await client.setex(cache_key, CACHE_TTL, json.dumps(stats))
            logger.debug(f"ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯: {group_id}")

        return stats

    except Exception as e:
        logger.error(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return {"error": str(e)}


async def _calculate_stats(group_id: str):
    """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # æ£€æŸ¥ç´¢å¼•çŠ¶æ€
        try:
            index_info = await client.execute_command('FT.INFO', group_id)
            index_status = "active"
        except:
            index_status = "not_found"

        # è·å–æ‰€æœ‰é—®é¢˜çš„key
        keys = await client.keys(f"{group_id}*")

        # ç»Ÿè®¡åˆ†ç±»å’Œå…¶ä»–ä¿¡æ¯
        categories = {}
        total_size = 0
        latest_update = None

        # ä½¿ç”¨pipelineæ‰¹é‡è·å–æ•°æ®
        if keys:
            pipe = client.pipeline()
            for key in keys:
                pipe.json().get(key, '$.category', '$.updated_at')

            results = await pipe.execute()

            for result in results:
                if result:
                    try:
                        category = result.get('$.category', ['æœªåˆ†ç±»'])[0] if result.get('$.category') else 'æœªåˆ†ç±»'
                        categories[category] = categories.get(category, 0) + 1

                        # è·Ÿè¸ªæœ€æ–°æ›´æ–°æ—¶é—´
                        updated_at = result.get('$.updated_at', [None])[0] if result.get('$.updated_at') else None
                        if updated_at and (not latest_update or updated_at > latest_update):
                            latest_update = updated_at

                    except (KeyError, IndexError, TypeError):
                        categories['æœªåˆ†ç±»'] = categories.get('æœªåˆ†ç±»', 0) + 1

        stats = {
            "total_questions": len(keys),
            "categories": categories,
            "index_status": index_status,
            "last_updated": latest_update,
            "cache_time": datetime.now().isoformat()
        }

        logger.debug(f"è®¡ç®—ç»Ÿè®¡ä¿¡æ¯å®Œæˆ: {group_id}")
        return stats

    except Exception as e:
        logger.error(f"è®¡ç®—ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return {"error": str(e)}


# æ¸…ç†å’Œç»´æŠ¤åŠŸèƒ½

async def cleanup_expired_cache():
    """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜"""
    try:
        # è·å–æ‰€æœ‰ç¼“å­˜é”®
        cache_keys = await client.keys("hotspot:*")
        cleaned = 0

        for key in cache_keys:
            ttl = await client.ttl(key)
            if ttl == -1:  # æ²¡æœ‰è¿‡æœŸæ—¶é—´è®¾ç½®
                await client.expire(key, CACHE_TTL)
                cleaned += 1

        logger.info(f"ç¼“å­˜æ¸…ç†å®Œæˆ: å¤„ç†äº† {cleaned} ä¸ªé”®")
        return cleaned

    except Exception as e:
        logger.error(f"ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
        return 0


async def get_hotspot_question(group_id: str, question_id: str):
    """è·å–çƒ­ç‚¹é—®é¢˜"""
    key = f"{group_id}{question_id}"
    try:
        result = await client.json().get(key)
        if result:
            logger.debug(f"âœ… è·å–é—®é¢˜: {question_id}")
        else:
            logger.debug(f"âš ï¸ é—®é¢˜ä¸å­˜åœ¨: {question_id}")
        return result
    except Exception as e:
        logger.error(f"âŒ è·å–å¤±è´¥: {e}")
        return None


async def list_all_questions(group_id: str = None, limit: int = 10, offset: int = 0):
    """è·å–é—®é¢˜åˆ—è¡¨ - æ”¯æŒåˆ†é¡µ"""
    try:
        if not group_id:
            return []

        query = "*"

        result = await client.execute_command(
            'FT.SEARCH', group_id, query,
            'LIMIT', str(offset), str(limit),
            'RETURN', '7', 'question', 'standard_reply', 'category',
            'question_id', 'related_links', 'created_at', 'updated_at'
        )

        parsed_results = parse_search_result(result)
        logger.debug(f"è·å–é—®é¢˜åˆ—è¡¨: {len(parsed_results)} ä¸ªé—®é¢˜")
        return parsed_results

    except Exception as e:
        logger.error(f"âŒ è·å–é—®é¢˜åˆ—è¡¨å¤±è´¥: {e}")
        return []


async def delete_hotspot_question(group_id: str, question_id: str):
    """åˆ é™¤çƒ­ç‚¹é—®é¢˜"""
    key = f"{group_id}{question_id}"
    try:
        result = await client.delete(key)
        if result > 0:
            # æ¸…é™¤ç›¸å…³ç¼“å­˜
            cache_key = STATS_CACHE_KEY.format(group_id=group_id)
            await client.delete(cache_key)
            logger.info(f"âœ… åˆ é™¤é—®é¢˜: {question_id}")
        else:
            logger.warning(f"âš ï¸ é—®é¢˜ä¸å­˜åœ¨: {question_id}")
        return result > 0
    except Exception as e:
        logger.error(f"âŒ åˆ é™¤å¤±è´¥: {e}")
        return False


async def delete_questions_by_category(group_id: str, category: str):
    """æŒ‰åˆ†ç±»åˆ é™¤é—®é¢˜"""
    try:
        result = await client.execute_command(
            'FT.SEARCH', group_id, f"@category:{{{category}}}",
            'RETURN', '0'
        )

        if len(result) < 2:
            logger.info(f"åˆ†ç±» {category} ä¸‹æ²¡æœ‰é—®é¢˜éœ€è¦åˆ é™¤")
            return 0

        # ä½¿ç”¨pipelineæ‰¹é‡åˆ é™¤
        pipe = client.pipeline()
        keys_to_delete = []

        for i in range(1, len(result), 2):
            key = result[i]
            keys_to_delete.append(key)
            pipe.delete(key)

        if keys_to_delete:
            results = await pipe.execute()
            count = sum(1 for r in results if r)

            # æ¸…é™¤ç¼“å­˜
            cache_key = STATS_CACHE_KEY.format(group_id=group_id)
            await client.delete(cache_key)

            logger.info(f"âœ… åˆ é™¤åˆ†ç±» {category}: {count} ä¸ªé—®é¢˜")
            return count

        return 0

    except Exception as e:
        logger.error(f"âŒ æŒ‰åˆ†ç±»åˆ é™¤å¤±è´¥: {e}")
        return 0